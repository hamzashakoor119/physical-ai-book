---
id: ch2-sensors-physical-ai
title: "Chapter 2: Sensors in Physical AI"
sidebar_label: "2. Sensors in Physical AI"
sidebar_position: 2
description: "Core technical chapter covering sensor types, working principles, noise and uncertainty, sensor fusion, calibration, and ROS 2 integration for Physical AI systems."
keywords: [sensors, lidar, imu, camera, depth sensing, sensor fusion, robotics, ros2]
---

# Chapter 2: Sensors in Physical AI

> *"A robot without sensors is like a human without senses—it cannot perceive, and therefore cannot intelligently act."*

---

## 1. Overview / Introduction

In Chapter 1, we established that Physical AI systems operate through continuous **perception-action loops**—sensing the environment, making decisions, acting, and observing consequences. This chapter dives deep into the **perception** side of that loop: **sensors**.

Sensors are the eyes, ears, and nervous system of Physical AI. They transform physical phenomena—light, sound, pressure, acceleration, magnetic fields—into electrical signals that computers can process. Without sensors, a robot is blind to its environment and deaf to feedback from its own body. It cannot navigate, manipulate, or interact intelligently with the physical world.

### What You Will Learn

By the end of this chapter, you will be able to:

- **Define sensors** and explain their role in Physical AI perception systems
- **Categorize sensors** as proprioceptive (internal state) or exteroceptive (environmental)
- **Explain working principles** of key sensors: LiDAR, cameras, IMUs, depth sensors, force/torque sensors
- **Understand noise and uncertainty**—why sensors are imperfect and how to handle it
- **Apply signal processing basics**: sampling, filtering, noise reduction
- **Describe sensor fusion** concepts and why combining sensors improves perception
- **Implement sensor subscribers** in ROS 2 and visualize data in RViz
- **Select appropriate sensors** for specific Physical AI tasks

### Prerequisites

| Concept | Level | Notes |
|---------|-------|-------|
| Chapter 1: Physical AI Introduction | **Required** | Perception-action loop, embodiment |
| Basic Physics | Recommended | Optics, mechanics (explained where needed) |
| Basic Math | Recommended | Algebra, vectors (no calculus required) |
| Python Programming | Optional | Helpful for code examples |
| ROS 2 Basics | Recommended | Topics, nodes, messages |

### Chapter Structure

```
1. Overview / Introduction
2. What Are Sensors? (Definition & Role)
3. Sensor Classification (Proprioceptive vs Exteroceptive)
4. Key Sensor Types (LiDAR, Camera, IMU, Depth, Force/Torque, GPS)
5. Sensor Characteristics (Range, Resolution, Accuracy, FOV)
6. Noise, Uncertainty & Error Sources
7. Signal Processing Fundamentals
8. Sensor Fusion Concepts
9. Calibration & Accuracy
10. Python Sensor Simulations
11. Real-World Robotics Applications
12. Chapter Summary
13. Glossary
14. Exercises / Review Questions
```

---

## 2. What Are Sensors?

### Definition

A **sensor** is a device that measures physical properties of the environment or the robot's internal state and converts them into electrical signals that can be processed by a computer.

More formally:

> **Sensor**: A transducer that converts a physical quantity (light intensity, distance, acceleration, force, temperature, etc.) into an electrical signal (voltage, current, digital value) suitable for measurement, processing, or control.

### The Role of Sensors in Physical AI

In the perception-action loop, sensors serve as the **input interface** between the physical world and the robot's computational system:

```
┌─────────────────────────────────────────────────────────────────┐
│                    PERCEPTION-ACTION LOOP                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   PHYSICAL WORLD                                                 │
│        │                                                         │
│        ▼ (light, sound, force, motion, distance)                │
│   ┌─────────┐                                                   │
│   │ SENSORS │ ◄─── Chapter 2 Focus                              │
│   └────┬────┘                                                   │
│        │ (electrical signals: voltage, bits)                    │
│        ▼                                                         │
│   ┌─────────────┐                                               │
│   │ PERCEPTION  │ (signal processing, feature extraction)       │
│   └──────┬──────┘                                               │
│          │                                                       │
│          ▼                                                       │
│   ┌─────────────┐     ┌───────────┐     ┌─────────────┐        │
│   │   DECISION  │────▶│ ACTUATORS │────▶│ ENVIRONMENT │        │
│   └─────────────┘     └───────────┘     └──────┬──────┘        │
│                                                 │                │
│                       (feedback loop)           │                │
│   ◄─────────────────────────────────────────────┘                │
└─────────────────────────────────────────────────────────────────┘
```

Sensors enable robots to:

1. **Perceive the environment**: Detect obstacles, recognize objects, map spaces
2. **Monitor internal state**: Track joint positions, body orientation, motor currents
3. **Close feedback loops**: Measure the effects of actions to enable error correction
4. **Ensure safety**: Detect collisions, limit forces, avoid hazards

### Sensors as Transducers

Sensors convert one form of energy to another. This conversion is called **transduction**:

| Physical Quantity | Sensor Type | Output Signal |
|-------------------|-------------|---------------|
| Light intensity | Photodiode, CCD | Voltage, digital image |
| Distance | LiDAR, ultrasonic | Time-of-flight → distance |
| Acceleration | Accelerometer | Voltage proportional to g-force |
| Angular velocity | Gyroscope | Voltage proportional to °/s |
| Force | Strain gauge | Resistance change → voltage |
| Magnetic field | Magnetometer | Voltage proportional to field strength |
| Temperature | Thermocouple | Voltage difference |

### Key Insight: Sensors Are Imperfect

A fundamental principle in Physical AI:

> **No sensor provides perfect measurements.** Every sensor output contains noise, bias, and uncertainty. Robust perception requires understanding these imperfections and designing systems that handle them gracefully.

We'll explore noise and uncertainty in detail in Section 6.

---

## 3. Sensor Classification

Sensors in Physical AI are broadly classified into two categories based on **what** they measure:

### 3.1 Proprioceptive Sensors (Internal State)

**Proprioceptive** sensors measure the robot's **internal state**—properties of the robot's own body rather than the external environment.

The term comes from biology: **proprioception** is the sense of body position and movement (knowing where your limbs are without looking).

| Sensor Type | Measures | Example Use |
|-------------|----------|-------------|
| **IMU** (Inertial Measurement Unit) | Acceleration, angular velocity, orientation | Balance, navigation |
| **Encoders** | Joint angles, wheel rotation | Motor control, odometry |
| **Force/Torque Sensors** | Contact forces, joint torques | Manipulation, safety |
| **Current Sensors** | Motor current | Effort estimation, fault detection |
| **Strain Gauges** | Mechanical deformation | Structural stress monitoring |

**Example**: A humanoid robot's IMU measures body tilt. This is proprioceptive because it senses the robot's internal orientation, not external objects.

### 3.2 Exteroceptive Sensors (External Environment)

**Exteroceptive** sensors measure the **external environment**—properties of the world outside the robot's body.

| Sensor Type | Measures | Example Use |
|-------------|----------|-------------|
| **LiDAR** | Distance to surfaces | Mapping, obstacle detection |
| **RGB Camera** | Light intensity (color images) | Object recognition, visual SLAM |
| **Depth Camera** | Distance per pixel | 3D reconstruction, manipulation |
| **Ultrasonic** | Distance (short range) | Proximity detection |
| **Radar** | Distance, velocity (radio waves) | Automotive, long range |
| **GPS** | Global position | Outdoor navigation |
| **Microphone** | Sound waves | Voice commands, acoustic localization |

**Example**: A self-driving car's LiDAR measures distances to surrounding vehicles and obstacles. This is exteroceptive because it senses the external environment.

### 3.3 Classification Diagram

```
                         SENSORS
                            │
            ┌───────────────┴───────────────┐
            │                               │
     PROPRIOCEPTIVE                   EXTEROCEPTIVE
     (Internal State)                 (Environment)
            │                               │
    ┌───────┼───────┐           ┌───────────┼───────────┐
    │       │       │           │           │           │
   IMU   Encoders  Force     LiDAR      Camera       GPS
    │               │           │           │
┌───┴───┐      Force/Torque  2D/3D    RGB/Depth/IR
│   │   │
Accel Gyro Mag
```

### 3.4 Active vs. Passive Sensors

Another classification dimension:

**Passive Sensors**: Detect naturally occurring signals (light, heat, sound)
- RGB cameras (ambient light)
- Microphones (sound waves)
- Infrared thermal cameras (heat radiation)

**Active Sensors**: Emit energy and measure the return signal
- LiDAR (emits laser pulses, measures reflection time)
- Ultrasonic (emits sound pulses, measures echo)
- Structured light depth cameras (project patterns)
- Radar (emits radio waves)

**Tradeoff**: Active sensors work in darkness but consume more power and can interfere with each other. Passive sensors are power-efficient but depend on ambient conditions.

---

## 4. Key Sensor Types in Physical AI

### 4.1 LiDAR (Light Detection and Ranging)

**Working Principle**: LiDAR emits laser pulses and measures the time for light to travel to a surface and return. Distance is calculated from the speed of light:

$$d = \frac\\{c \cdot t\\}\\{2\\}$$

Where:
- $d$ = distance to surface (meters)
- $c$ = speed of light (≈ 3 × 10⁸ m/s)
- $t$ = round-trip time (seconds)

**Types of LiDAR**:

| Type | Description | Output | Applications |
|------|-------------|--------|--------------|
| **2D LiDAR** | Single rotating laser, horizontal plane | LaserScan (distances at angles) | Mobile robot navigation |
| **3D LiDAR** | Multiple lasers or spinning array | Point cloud (x, y, z per point) | Autonomous vehicles, mapping |
| **Solid-State LiDAR** | No moving parts, electronic steering | Point cloud | Automotive (emerging) |

**Diagram: 2D LiDAR Operation**

```
              Rotating Laser Scanner
                     │
        ◄────────────┼────────────►
       /             │              \
      /              │               \
     /    Laser      │     Laser      \
    /     Beam       │      Beam       \
   /                 │                  \
  ▼                  ▼                   ▼
┌────┐            ┌────┐             ┌────┐
│Wall│            │Robot│            │Wall│
└────┘            └────┘             └────┘

         ◄───── Field of View ─────►
              (typically 270°)

Output: Array of distances at each angle
        [d₀, d₁, d₂, ..., dₙ]
        angle_min to angle_max
```

**Key Specifications**:

| Parameter | Typical Values | Impact |
|-----------|----------------|--------|
| Range | 0.1m – 100m+ | Detection distance |
| Angular resolution | 0.25° – 1° | Point density |
| Scan rate | 10 – 40 Hz | Update frequency |
| Field of view | 180° – 360° | Coverage area |
| Accuracy | ±2 – 5 cm | Measurement precision |

**Noise Sources**:
- Reflective surfaces (glass, mirrors) → incorrect readings
- Dark surfaces (black materials) → weak returns
- Rain, fog, dust → scattering
- Multi-path reflections → ghost points
- Maximum range limitations

**ROS 2 Message Type**: `sensor_msgs/LaserScan` (2D), `sensor_msgs/PointCloud2` (3D)

---

### 4.2 RGB Cameras

**Working Principle**: Cameras capture light through a lens, focusing it onto a 2D array of photosensitive pixels (CCD or CMOS sensor). Each pixel measures light intensity; color cameras use filters (Bayer pattern) to separate red, green, and blue channels.

**Pinhole Camera Model**:

The fundamental mathematical model relates 3D world points to 2D image coordinates:

$$\begin\\{bmatrix\\} u \\ v \\ 1 \end\\{bmatrix\\} = \frac\\{1\\}\\{Z\\} \begin\\{bmatrix\\} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end\\{bmatrix\\} \begin\\{bmatrix\\} X \\ Y \\ Z \end\\{bmatrix\\}$$

Where:
- $(X, Y, Z)$ = 3D point in camera frame
- $(u, v)$ = pixel coordinates in image
- $f_x, f_y$ = focal lengths (pixels)
- $(c_x, c_y)$ = principal point (image center)

**Diagram: Camera Projection**

```
        3D World Point (X, Y, Z)
                 *
                /|\
               / | \
              /  |  \
             /   |   \
            /    |    \
           /     |     \
          /      |      \
    ─────/───────┼───────\─────  Image Plane
        │    (u,v)       │
        │       •        │       2D Image
        │                │
        └────────────────┘

    Focal Length (f) determines magnification
    Principal Point (cx, cy) is image center
```

**Key Specifications**:

| Parameter | Typical Values | Impact |
|-----------|----------------|--------|
| Resolution | 640×480 to 4K+ | Detail, processing load |
| Frame rate | 30 – 120+ fps | Motion blur, latency |
| Field of view | 60° – 180° (fisheye) | Coverage vs distortion |
| Dynamic range | 60 – 120 dB | Low-light performance |
| Color depth | 8 – 12 bits/channel | Color accuracy |

**Noise Sources**:
- Low light → high noise (grain)
- Motion blur → fast movement
- Over/under exposure → clipped values
- Lens distortion → geometric errors
- Rolling shutter → skew with fast motion

**ROS 2 Message Type**: `sensor_msgs/Image`, `sensor_msgs/CompressedImage`

---

### 4.3 Depth Cameras

**Purpose**: Provide per-pixel distance measurements, creating a 3D representation of the scene.

**Three Main Technologies**:

#### Stereo Vision
- **How it works**: Two cameras separated by a known baseline capture images. Triangulation from pixel disparities yields depth.
- **Advantages**: Works outdoors in sunlight, passive (no emitter)
- **Disadvantages**: Requires texture for matching, computationally intensive
- **Example**: Intel RealSense D400 series, ZED cameras

$$Z = \frac\\{f \cdot B\\}\\{d\\}$$

Where:
- $Z$ = depth
- $f$ = focal length
- $B$ = baseline (camera separation)
- $d$ = disparity (pixel difference between images)

#### Structured Light
- **How it works**: Projects a known pattern (dots, stripes) onto the scene. Deformation of the pattern encodes depth.
- **Advantages**: Dense depth, works on textureless surfaces
- **Disadvantages**: Fails in sunlight (pattern washed out), limited range
- **Example**: Intel RealSense D400 (with projector), Microsoft Kinect v1

#### Time-of-Flight (ToF)
- **How it works**: Emits modulated infrared light, measures phase shift of the return signal to calculate distance.
- **Advantages**: Fast, dense depth, works indoors/outdoors
- **Disadvantages**: Multi-path interference, lower resolution
- **Example**: Microsoft Kinect v2, PMD cameras

**Comparison Table**:

| Technology | Range | Outdoor Use | Texture Needed | Processing |
|------------|-------|-------------|----------------|------------|
| Stereo | 0.5–20m | ✅ Yes | Yes | High |
| Structured Light | 0.3–5m | ❌ No | No | Medium |
| Time-of-Flight | 0.1–10m | ✅ Yes | No | Low |

**ROS 2 Message Type**: `sensor_msgs/Image` (depth as 16-bit), `sensor_msgs/PointCloud2`

---

### 4.4 IMU (Inertial Measurement Unit)

**Components**: An IMU combines multiple inertial sensors:

| Component | Measures | Units |
|-----------|----------|-------|
| **Accelerometer** | Linear acceleration | m/s² or g |
| **Gyroscope** | Angular velocity | °/s or rad/s |
| **Magnetometer** (optional) | Magnetic field | μT or Gauss |

**6-DOF vs 9-DOF IMU**:
- **6-DOF**: 3-axis accelerometer + 3-axis gyroscope
- **9-DOF**: 6-DOF + 3-axis magnetometer (heading reference)

**Working Principles**:

**Accelerometer** (MEMS):
- Tiny proof mass suspended by springs
- Acceleration causes mass displacement
- Capacitive sensing measures displacement

```
    ┌─────────────────────────┐
    │    Fixed Electrodes     │
    │  ═══════════════════    │
    │         │││             │
    │    ┌────┴┴┴────┐       │  ← Proof Mass
    │    │  ███████  │        │    (moves with acceleration)
    │    └───────────┘        │
    │         │││             │
    │  ═══════════════════    │
    │    Fixed Electrodes     │
    └─────────────────────────┘

    Acceleration → Mass moves → Capacitance changes → Voltage output
```

**Gyroscope** (MEMS):
- Vibrating structure (tuning fork or ring)
- Rotation causes Coriolis force
- Perpendicular displacement measured capacitively

**Key Specifications**:

| Parameter | Typical Values | Impact |
|-----------|----------------|--------|
| Range (accel) | ±2g to ±16g | Maximum measurable acceleration |
| Range (gyro) | ±250°/s to ±2000°/s | Maximum rotation rate |
| Noise density | 0.1 – 1 mg/√Hz | Measurement noise |
| Bias stability | 0.01 – 10 °/hr | Drift over time |
| Update rate | 100 – 8000 Hz | Sampling frequency |

**Critical Issue: Gyroscope Drift**

Gyroscopes measure angular velocity, not absolute angle. To get orientation, you must integrate:

$$\theta(t) = \theta_0 + \int_0^t \omega(\tau) \, d\tau$$

**Problem**: Any bias error in $\omega$ accumulates over time:
- Bias of 0.1°/s → 6°/minute → 360°/hour!

**Solution**: Sensor fusion (Section 8) combines gyroscope with accelerometer and/or magnetometer to correct drift.

**ROS 2 Message Type**: `sensor_msgs/Imu`

---

### 4.5 Force/Torque Sensors

**Purpose**: Measure contact forces and torques, essential for manipulation and safe human-robot interaction.

**Working Principle**: Most use strain gauges—resistors that change resistance when deformed:

$$\frac\\{\Delta R\\}\\{R\\} = GF \cdot \epsilon$$

Where:
- $\Delta R$ = resistance change
- $GF$ = gauge factor (typically 2)
- $\epsilon$ = strain (deformation)

**6-Axis Force/Torque Sensor**:
Measures forces ($F_x, F_y, F_z$) and torques ($\tau_x, \tau_y, \tau_z$) in all directions.

```
              Z (vertical)
              ↑
              │    τz (twist)
              │   ↺
        ┌─────┼─────┐
        │     │     │
   ←────┼─────┼─────┼────→ X
   Fx   │     │     │
        │     │     │
        └─────┼─────┘
              │
              ↓ Fz (compression/tension)

        ↙ Y (into page)
```

**Key Specifications**:

| Parameter | Typical Values |
|-----------|----------------|
| Force range | 10 N – 10 kN |
| Torque range | 0.1 Nm – 100 Nm |
| Resolution | 0.01% of full scale |
| Overload capacity | 150-500% of rated |
| Sampling rate | 100 Hz – 10 kHz |

**Applications**:
- Robotic manipulation (grasp force control)
- Collision detection (safety)
- Assembly tasks (force-guided insertion)
- Haptic feedback

**ROS 2 Message Type**: `geometry_msgs/Wrench`, `geometry_msgs/WrenchStamped`

---

### 4.6 GPS (Global Positioning System)

**Working Principle**: GPS receivers measure signals from multiple satellites (minimum 4) and use trilateration to determine position.

Each satellite broadcasts:
1. Its precise orbital position (ephemeris)
2. A timing signal synchronized to atomic clocks

The receiver calculates distance to each satellite from signal travel time, then solves for its position.

**Accuracy Levels**:

| GPS Type | Accuracy | Use Case |
|----------|----------|----------|
| Standard GPS | 2 – 5 m | Consumer devices |
| DGPS (Differential) | 0.5 – 2 m | Agriculture, marine |
| RTK (Real-Time Kinematic) | 1 – 2 cm | Surveying, drones, robotics |
| PPP (Precise Point) | 5 – 10 cm | Precision agriculture |

**Limitations**:
- Requires clear sky view (fails indoors, urban canyons)
- Multipath errors (signal reflections)
- Atmospheric delays
- Satellite geometry affects precision

**ROS 2 Message Type**: `sensor_msgs/NavSatFix`

---

### 4.7 Sensor Comparison Summary

| Sensor | What it Measures | Range | Update Rate | Indoor/Outdoor | Cost |
|--------|------------------|-------|-------------|----------------|------|
| 2D LiDAR | Distance (plane) | 0.1–30m | 10–40 Hz | Both | $100–$5K |
| 3D LiDAR | Distance (3D) | 0.5–200m | 10–20 Hz | Both | $1K–$75K |
| RGB Camera | Color image | N/A | 30–120 fps | Both | $20–$500 |
| Depth Camera | Per-pixel depth | 0.3–10m | 30–90 fps | Indoors mainly | $150–$1K |
| IMU | Accel/gyro/mag | N/A | 100–8000 Hz | Both | $5–$5K |
| Force/Torque | Forces, torques | Varies | 100–10K Hz | Both | $500–$10K |
| GPS | Global position | Global | 1–20 Hz | Outdoors only | $20–$5K |
| Ultrasonic | Short-range distance | 0.02–4m | 10–50 Hz | Both | $5–$50 |

---

## 5. Sensor Characteristics

When selecting sensors for a Physical AI system, engineers evaluate several key characteristics:

### 5.1 Range

**Definition**: The minimum and maximum distances (or values) a sensor can measure.

**Examples**:
- LiDAR: 0.1m minimum (blind zone), 100m maximum
- Ultrasonic: 2cm – 4m typical
- IMU accelerometer: ±2g, ±4g, ±8g, ±16g selectable

**Design Consideration**: Choose range appropriate for your application. Higher range often means lower resolution or accuracy.

### 5.2 Resolution

**Definition**: The smallest change in the measured quantity that the sensor can detect.

**Examples**:
- LiDAR angular resolution: 0.25° between measurements
- Camera: 1920×1080 pixels (spatial resolution)
- Depth camera: 1mm depth resolution
- IMU: 12-bit ADC → 4096 discrete levels

**Formula for digital sensors**:
$$\text\\{Resolution\\} = \frac\\{\text\\{Range\\}\\}\\{2^\\{n\\}\\}$$

Where $n$ = number of bits in the ADC.

### 5.3 Accuracy vs. Precision

Two often-confused concepts:

**Accuracy**: How close measurements are to the true value (systematic error).

**Precision**: How repeatable measurements are (random error/noise).

```
     High Accuracy           Low Accuracy           Low Accuracy
     High Precision          High Precision         Low Precision

        ●  ●                    ●  ●                 ●
       ●    ●                  ●    ●                    ●
        ●  ●                    ●  ●              ●
          ⊕                          ⊕              ⊕   ●
     (shots clustered        (shots clustered      (shots scattered
      around center)          away from center)     everywhere)

     IDEAL                   CALIBRATION NEEDED    NOISY SENSOR
```

**Key Insight**: A precise but inaccurate sensor can often be calibrated. An imprecise sensor is fundamentally limited by noise.

### 5.4 Field of View (FOV)

**Definition**: The angular extent of the environment visible to the sensor.

| Sensor | Typical FOV |
|--------|-------------|
| 2D LiDAR | 180° – 360° horizontal |
| 3D LiDAR | 360° × 30° (H × V) |
| Standard camera | 60° – 90° |
| Fisheye camera | 150° – 180°+ |
| Depth camera | 60° – 90° |

**Tradeoff**: Wider FOV means more coverage but lower angular resolution (same number of pixels spread over larger area).

### 5.5 Sampling Rate (Update Frequency)

**Definition**: How many measurements per second the sensor provides.

**Critical Concept: Nyquist Theorem**

To accurately capture a signal that changes at frequency $f$, you must sample at least $2f$:

$$f_s \geq 2 \cdot f_\\{max\\}$$

**Example**: A robot arm moves at up to 10 Hz (cycles per second). Encoders must sample at least 20 Hz—practically 100+ Hz for good control.

**Sampling Rate Requirements by Application**:

| Application | Required Rate | Reason |
|-------------|---------------|--------|
| Humanoid balance | 1000+ Hz | Fast dynamics |
| Drone stabilization | 400–1000 Hz | Aerodynamic instability |
| Arm manipulation | 100–500 Hz | Contact forces |
| Mobile navigation | 10–50 Hz | Slower dynamics |

### 5.6 Latency

**Definition**: Time delay between the physical event and the sensor data being available.

**Sources of latency**:
1. **Sensor latency**: Integration time, processing
2. **Communication latency**: Data transfer (USB, Ethernet)
3. **Processing latency**: Compute time for algorithms

**Impact**: High latency makes feedback control unstable. A 100ms delay can make drone flight impossible.

**Typical Latencies**:

| Sensor | Typical Latency |
|--------|-----------------|
| IMU | 1–10 ms |
| Camera | 10–50 ms |
| LiDAR | 20–100 ms |
| Depth camera | 30–100 ms |
| GPS | 100–500 ms |

---

## 6. Noise, Uncertainty, and Error Sources

### 6.1 Why Sensors Are Imperfect

Every sensor measurement contains errors. Understanding error sources is essential for robust perception.

**Fundamental Truth**:
> The true value of any physical quantity is **unknowable**. We can only measure it with finite precision, and every measurement includes some error.

### 6.2 Types of Sensor Errors

#### Systematic Errors (Bias)

**Definition**: Consistent, repeatable errors that shift all measurements in the same direction.

**Examples**:
- IMU accelerometer reads 0.05 m/s² when stationary
- Camera reports colors warmer than reality (white balance off)
- LiDAR consistently measures 2cm too far

**Solution**: Calibration can measure and correct systematic errors.

#### Random Errors (Noise)

**Definition**: Unpredictable fluctuations in measurements that vary randomly.

**Examples**:
- Accelerometer output jitters around the true value
- Camera pixels have shot noise (random variations)
- LiDAR distance varies slightly between scans

**Characteristics**:
- Often modeled as Gaussian (normal distribution)
- Cannot be eliminated, only reduced by averaging or filtering

#### Drift

**Definition**: Slow accumulation of error over time.

**Primary Example**: Gyroscope integration drift
- Small bias in angular velocity measurement
- Integrated over time → growing orientation error
- Can reach 360° error per hour for consumer IMUs

**Solution**: Sensor fusion with absolute references (accelerometer for tilt, magnetometer for heading, GPS for position).

### 6.3 Noise Models

Most sensor noise is modeled as **Gaussian (Normal) distribution**:

$$p(x) = \frac\\{1\\}\\{\sqrt\\{2\pi\sigma^2\\}\\} \exp\left(-\frac\\{(x-\mu)^2\\}\\{2\sigma^2\\}\right)$$

Where:
- $\mu$ = mean (true value + bias)
- $\sigma$ = standard deviation (noise magnitude)

**68-95-99.7 Rule**:
- 68% of measurements within ±1σ of mean
- 95% within ±2σ
- 99.7% within ±3σ

```
                    Gaussian Distribution

     │                    ▲
     │                   ╱ ╲
     │                  ╱   ╲
     │                 ╱     ╲
     │                ╱       ╲
     │               ╱         ╲
     │              ╱           ╲
     │             ╱             ╲
     │            ╱               ╲
     │           ╱                 ╲
     │──────────╱───────────────────╲──────────
              -3σ  -2σ  -σ   μ   σ   2σ  3σ

     True value is μ; measurements spread with std dev σ
```

### 6.4 Common Noise Sources by Sensor

| Sensor | Noise Sources | Typical σ |
|--------|---------------|-----------|
| **LiDAR** | Surface reflectivity, multipath, beam divergence | 2–5 cm |
| **Camera** | Shot noise (photons), read noise, thermal noise | Varies with exposure |
| **IMU (accel)** | Thermal noise, vibration | 0.001–0.01 g |
| **IMU (gyro)** | Angle random walk, bias instability | 0.01–1 °/s |
| **Depth camera** | Quantization, interference, edges | 1–3 cm |
| **GPS** | Multipath, atmospheric, satellite geometry | 2–5 m |

### 6.5 Signal-to-Noise Ratio (SNR)

**Definition**: Ratio of signal power to noise power, often in decibels (dB):

$$SNR_\\{dB\\} = 10 \log_\\{10\\}\left(\frac\\{P_\\{signal\\}\\}\\{P_\\{noise\\}\\}\right) = 20 \log_\\{10\\}\left(\frac\\{A_\\{signal\\}\\}\\{A_\\{noise\\}\\}\right)$$

**Interpretation**:
- SNR = 20 dB → signal is 10× stronger than noise
- SNR = 40 dB → signal is 100× stronger than noise
- Higher SNR = cleaner measurements

---

## 7. Signal Processing Fundamentals

### 7.1 Sampling

**Analog-to-Digital Conversion (ADC)**:

Real-world signals are continuous. Digital systems require discrete samples:

```
Continuous Signal              Sampled Signal (Discrete)

     ●                              ●
    ╱ ╲                            │
   ╱   ╲    ●                      │  ●
  ╱     ╲  ╱ ╲                     │ │
 ╱       ╲╱   ╲    ●               │ │   ●
╱              ╲  ╱                │ │  │
                ╲╱                 │ │  │
─────────────────────         ─┬──┬─┬──┬─┬──
     Time (continuous)          Ts  Ts  Ts
                               (Sample Period)
```

**Sampling Rate** ($f_s$): Number of samples per second (Hz).

**Nyquist-Shannon Theorem**:

To reconstruct a signal with maximum frequency $f_\\{max\\}$, sample at:

$$f_s > 2 \cdot f_\\{max\\}$$

**Aliasing**: Sampling below Nyquist causes high frequencies to appear as lower frequencies (false signals).

**Example**: A robot vibrating at 100 Hz sampled at only 60 Hz would appear to vibrate at a phantom 40 Hz.

### 7.2 Filtering

Filters remove unwanted components from signals—typically noise or frequencies outside the range of interest.

#### Low-Pass Filter

**Purpose**: Remove high-frequency noise, keep slow-changing signals.

**Simple Moving Average**:

$$y[n] = \frac\\{1\\}\\{N\\} \sum_\\{i=0\\}^\\{N-1\\} x[n-i]$$

**Exponential Moving Average (EMA)**:

$$y[n] = \alpha \cdot x[n] + (1-\alpha) \cdot y[n-1]$$

Where $\alpha$ (0 to 1) controls smoothing. Smaller α = more smoothing.

```
Raw Signal with Noise          After Low-Pass Filter

  ●    ●                           ●●●
 ● ●  ● ●  ●                      ●   ●●
●   ●●   ●● ●●                   ●      ●●
           ●  ●  ●              ●         ●●
               ●● ●                         ●●
                  ●●                          ●

(High-frequency jitter)        (Smooth trend extracted)
```

#### High-Pass Filter

**Purpose**: Remove slow-changing components (bias, drift), keep fast changes.

Useful for detecting sudden events (impacts, motion onset) while ignoring DC offsets.

### 7.3 Derivative and Integration

**Derivative** (rate of change):

$$\dot\\{x\\}[n] \approx \frac\\{x[n] - x[n-1]\\}\\{T_s\\}$$

- Amplifies noise (noise has high-frequency components)
- Used: Velocity from position, angular velocity from angle

**Integration** (accumulation):

$$x[n] = x[n-1] + \dot\\{x\\}[n] \cdot T_s$$

- Accumulates errors (drift)
- Used: Position from velocity, orientation from angular velocity

**Key Tradeoff**: Differentiation amplifies noise; integration accumulates drift. Both require careful filtering.

---

## 8. Sensor Fusion Concepts

### 8.1 Why Fuse Sensors?

No single sensor is perfect for all situations. **Sensor fusion** combines multiple sensors to achieve:

1. **Complementary Information**: Different sensors measure different things
2. **Redundancy**: If one sensor fails, others provide backup
3. **Improved Accuracy**: Averaging reduces noise
4. **Drift Correction**: Absolute sensors correct relative sensor drift

### 8.2 Example: IMU Sensor Fusion

An IMU has accelerometer and gyroscope, each with complementary strengths and weaknesses:

| Sensor | Measures | Strength | Weakness |
|--------|----------|----------|----------|
| Accelerometer | Gravity direction (tilt) | No drift (absolute reference) | Noisy, affected by motion |
| Gyroscope | Angular velocity | Smooth, fast | Drifts over time |

**Solution**: Fuse them!
- Short-term: Trust gyroscope (smooth)
- Long-term: Trust accelerometer (no drift)

### 8.3 Complementary Filter

The simplest sensor fusion approach for IMU orientation:

$$\theta_\\{fused\\} = \alpha \cdot (\theta_\\{prev\\} + \omega \cdot dt) + (1-\alpha) \cdot \theta_\\{accel\\}$$

Where:
- $\theta_\\{prev\\}$ = previous orientation estimate
- $\omega$ = gyroscope angular velocity
- $dt$ = time step
- $\theta_\\{accel\\}$ = orientation from accelerometer (gravity direction)
- $\alpha$ = filter coefficient (typically 0.95–0.99)

**Interpretation**:
- 95% weight on gyroscope (smooth, fast changes)
- 5% weight on accelerometer (corrects drift)

```
               Complementary Filter

   Gyroscope ────► Integrate ────►(High Freq)────┐
        ω            θ_gyro                      │
                                                 │
                                            ┌────┴────┐
                                            │   SUM   │────► θ_fused
                                            └────┬────┘
                                                 │
Accelerometer ────► Compute ────►(Low Freq) ────┘
      a             θ_accel

   High-pass filter on gyro (keeps fast changes)
   Low-pass filter on accel (keeps slow/DC reference)
```

### 8.4 Kalman Filter (Conceptual)

The **Kalman Filter** is the optimal estimator for linear systems with Gaussian noise. It's widely used in robotics for state estimation.

**Key Idea**: Maintain a probabilistic belief about the state, update it with predictions (from models) and corrections (from sensors).

**Two Steps**:

1. **Predict**: Use a motion model to predict the next state
   $$\hat\\{x\\}_\\{k|k-1\\} = F \cdot \hat\\{x\\}_\\{k-1\\} + B \cdot u_k$$
   $$P_\\{k|k-1\\} = F \cdot P_\\{k-1\\} \cdot F^T + Q$$

2. **Update**: Correct the prediction using sensor measurements
   $$K_k = P_\\{k|k-1\\} \cdot H^T \cdot (H \cdot P_\\{k|k-1\\} \cdot H^T + R)^\\{-1\\}$$
   $$\hat\\{x\\}_k = \hat\\{x\\}_\\{k|k-1\\} + K_k \cdot (z_k - H \cdot \hat\\{x\\}_\\{k|k-1\\})$$
   $$P_k = (I - K_k \cdot H) \cdot P_\\{k|k-1\\}$$

**Intuition**:
- $K_k$ is the **Kalman gain**—how much to trust the measurement vs. the prediction
- If sensor is accurate (low R), trust measurement more
- If prediction is accurate (low Q), trust prediction more

**When to Use**:
- GPS + IMU fusion for navigation
- LiDAR + odometry for localization
- Multi-camera tracking

### 8.5 Sensor Fusion Architecture

```
    ┌──────────────────────────────────────────────────────┐
    │                  SENSOR FUSION SYSTEM                │
    ├──────────────────────────────────────────────────────┤
    │                                                      │
    │   ┌─────────┐  ┌─────────┐  ┌─────────┐            │
    │   │  LiDAR  │  │ Camera  │  │   IMU   │            │
    │   └────┬────┘  └────┬────┘  └────┬────┘            │
    │        │            │            │                  │
    │        ▼            ▼            ▼                  │
    │   ┌─────────────────────────────────────┐          │
    │   │         PREPROCESSING               │          │
    │   │  (noise filtering, calibration)     │          │
    │   └────────────────┬────────────────────┘          │
    │                    │                                │
    │                    ▼                                │
    │   ┌─────────────────────────────────────┐          │
    │   │         FUSION ALGORITHM            │          │
    │   │   (Kalman Filter, EKF, Particle)    │          │
    │   └────────────────┬────────────────────┘          │
    │                    │                                │
    │                    ▼                                │
    │   ┌─────────────────────────────────────┐          │
    │   │         FUSED STATE ESTIMATE        │          │
    │   │   (position, velocity, orientation) │          │
    │   └─────────────────────────────────────┘          │
    │                                                      │
    └──────────────────────────────────────────────────────┘
```

---

## 9. Calibration & Accuracy

### 9.1 Why Calibrate?

**Calibration** measures and corrects systematic errors. Without calibration, even expensive sensors produce inaccurate measurements.

### 9.2 Camera Calibration

#### Intrinsic Parameters

Parameters internal to the camera that define how 3D points project to 2D pixels:

| Parameter | Symbol | Description |
|-----------|--------|-------------|
| Focal length | $f_x, f_y$ | Effective focal length in pixels |
| Principal point | $c_x, c_y$ | Image center (optical axis intersection) |
| Distortion | $k_1, k_2, p_1, p_2, k_3$ | Radial and tangential lens distortion |

**Intrinsic Matrix** (K):

$$K = \begin\\{bmatrix\\} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end\\{bmatrix\\}$$

#### Extrinsic Parameters

Position and orientation of the camera in the world frame:

- **Rotation matrix** $R$ (3×3): Camera orientation
- **Translation vector** $t$ (3×1): Camera position

#### Calibration Procedure

1. Print a checkerboard pattern with known square size
2. Capture images from multiple angles
3. Detect checkerboard corners in each image
4. Solve for intrinsic and extrinsic parameters

**Tools**: OpenCV `calibrateCamera()`, ROS camera_calibration package

### 9.3 IMU Calibration

**Accelerometer Calibration**:
- Measure at known orientations (0g, ±1g on each axis)
- Compute bias (offset) and scale factor
- Correct: $a_\\{true\\} = scale \cdot (a_\\{raw\\} - bias)$

**Gyroscope Calibration**:
- Keep IMU stationary, record output
- Average reading is the bias
- Subtract bias from all future readings

**Temperature Compensation**: IMU parameters drift with temperature. Advanced calibration includes temperature models.

### 9.4 Multi-Sensor Calibration

When using multiple sensors, their relative positions and orientations must be known (**extrinsic calibration**):

**LiDAR-Camera Calibration**:
- Align 3D LiDAR points with 2D camera image
- Find transformation that projects LiDAR points onto correct image pixels
- Critical for sensor fusion and data association

**IMU-Camera Calibration**:
- Determine IMU position/orientation relative to camera
- Required for visual-inertial odometry (VIO)

---

## 10. Python Sensor Simulations

### 10.1 Simulating Sensor Noise

```python
"""
Sensor Noise Simulation
Demonstrates how noise affects sensor readings
"""

import random
import math

def simulate_sensor(true_value, noise_std, bias=0.0):
    """
    Simulate a noisy sensor reading.

    Args:
        true_value: The actual physical quantity
        noise_std: Standard deviation of Gaussian noise
        bias: Systematic error (constant offset)

    Returns:
        Simulated sensor reading with noise and bias
    """
    noise = random.gauss(0, noise_std)
    return true_value + bias + noise


# Simulate LiDAR distance measurement
true_distance = 5.0  # meters
lidar_noise_std = 0.03  # 3 cm standard deviation
lidar_bias = 0.02  # 2 cm systematic error

print("=== LiDAR Distance Simulation ===")
print(f"True distance: {true_distance:.3f} m")
print(f"Noise std: {lidar_noise_std*100:.1f} cm, Bias: {lidar_bias*100:.1f} cm")
print("-" * 40)

readings = []
for i in range(10):
    reading = simulate_sensor(true_distance, lidar_noise_std, lidar_bias)
    readings.append(reading)
    error = reading - true_distance
    print(f"Reading {i+1:2d}: {reading:.4f} m  (error: {error*100:+.2f} cm)")

# Statistics
avg = sum(readings) / len(readings)
variance = sum((r - avg)**2 for r in readings) / len(readings)
std = math.sqrt(variance)

print("-" * 40)
print(f"Average: {avg:.4f} m  (bias visible: {(avg-true_distance)*100:.2f} cm)")
print(f"Std Dev: {std*100:.2f} cm  (close to noise_std: {lidar_noise_std*100:.1f} cm)")
```

**Expected Output**:
```
=== LiDAR Distance Simulation ===
True distance: 5.000 m
Noise std: 3.0 cm, Bias: 2.0 cm
----------------------------------------
Reading  1: 5.0523 m  (error: +5.23 cm)
Reading  2: 5.0087 m  (error: +0.87 cm)
...
Average: 5.0198 m  (bias visible: 1.98 cm)
Std Dev: 2.89 cm  (close to noise_std: 3.0 cm)
```

### 10.2 Sampling and Aliasing Demonstration

```python
"""
Nyquist Sampling Demonstration
Shows what happens when sampling rate is too low
"""

import math

def generate_sine_wave(frequency, duration, sample_rate):
    """Generate a sampled sine wave."""
    samples = []
    num_samples = int(duration * sample_rate)
    for i in range(num_samples):
        t = i / sample_rate
        value = math.sin(2 * math.pi * frequency * t)
        samples.append((t, value))
    return samples


# Signal frequency: 10 Hz
signal_freq = 10  # Hz

print("=== Nyquist Sampling Demonstration ===")
print(f"Signal frequency: {signal_freq} Hz")
print(f"Nyquist rate: {2 * signal_freq} Hz (minimum)")
print()

# Test different sampling rates
for sample_rate in [50, 25, 15, 8]:
    samples = generate_sine_wave(signal_freq, 0.2, sample_rate)

    # Check if we can distinguish the wave
    nyquist_ok = sample_rate >= 2 * signal_freq

    print(f"Sample rate: {sample_rate:2d} Hz  ", end="")
    print(f"Samples/cycle: {sample_rate/signal_freq:.1f}  ", end="")

    if nyquist_ok:
        print("✓ Adequate (above Nyquist)")
    else:
        # Calculate apparent (aliased) frequency
        apparent_freq = abs(signal_freq - sample_rate * round(signal_freq / sample_rate))
        print(f"✗ ALIASING! Appears as {apparent_freq} Hz")

print()
print("Lesson: Always sample at least 2x the highest frequency!")
```

### 10.3 Simple Low-Pass Filter

```python
"""
Low-Pass Filter Implementation
Demonstrates noise reduction through filtering
"""

import random

def low_pass_filter_ema(data, alpha):
    """
    Exponential Moving Average low-pass filter.

    Args:
        data: List of raw sensor readings
        alpha: Smoothing factor (0-1). Smaller = more smoothing.

    Returns:
        Filtered data
    """
    filtered = [data[0]]  # Initialize with first value
    for i in range(1, len(data)):
        # EMA formula: y[n] = alpha * x[n] + (1-alpha) * y[n-1]
        new_value = alpha * data[i] + (1 - alpha) * filtered[-1]
        filtered.append(new_value)
    return filtered


# Generate noisy sensor data
true_value = 10.0
noise_std = 2.0
n_samples = 20

raw_data = [true_value + random.gauss(0, noise_std) for _ in range(n_samples)]

# Apply filter with different alpha values
filtered_light = low_pass_filter_ema(raw_data, alpha=0.5)  # Light smoothing
filtered_heavy = low_pass_filter_ema(raw_data, alpha=0.1)  # Heavy smoothing

print("=== Low-Pass Filter Demonstration ===")
print(f"True value: {true_value}, Noise std: {noise_std}")
print()
print(f"{'Sample':>6} | {'Raw':>8} | {'α=0.5':>8} | {'α=0.1':>8}")
print("-" * 42)

for i in range(n_samples):
    print(f"{i:6d} | {raw_data[i]:8.2f} | {filtered_light[i]:8.2f} | {filtered_heavy[i]:8.2f}")

# Calculate final errors
raw_error = abs(sum(raw_data[-5:])/5 - true_value)
light_error = abs(sum(filtered_light[-5:])/5 - true_value)
heavy_error = abs(sum(filtered_heavy[-5:])/5 - true_value)

print("-" * 42)
print(f"Final avg error: Raw={raw_error:.2f}, α=0.5: {light_error:.2f}, α=0.1: {heavy_error:.2f}")
print()
print("Note: Heavy filtering (α=0.1) is smoother but responds slower to changes!")
```

### 10.4 Complementary Filter for IMU

```python
"""
Complementary Filter for IMU Orientation Estimation
Fuses accelerometer and gyroscope for tilt estimation
"""

import math
import random

def simulate_imu(true_angle, gyro_bias, gyro_noise, accel_noise):
    """Simulate IMU readings."""
    # Gyroscope: measures angular velocity with bias and noise
    true_angular_velocity = 0  # Stationary for this demo
    gyro_reading = true_angular_velocity + gyro_bias + random.gauss(0, gyro_noise)

    # Accelerometer: measures gravity direction with noise
    # For small angles: angle ≈ atan2(ax, az) ≈ ax/g
    accel_angle = true_angle + random.gauss(0, accel_noise)

    return gyro_reading, accel_angle


def complementary_filter(gyro_angle, accel_angle, alpha):
    """
    Complementary filter combining gyroscope and accelerometer.

    Args:
        gyro_angle: Angle from integrated gyroscope
        accel_angle: Angle from accelerometer (gravity reference)
        alpha: Weight for gyroscope (0.9-0.99 typical)

    Returns:
        Fused angle estimate
    """
    return alpha * gyro_angle + (1 - alpha) * accel_angle


# Simulation parameters
true_angle = 15.0  # degrees (robot tilted 15°)
dt = 0.01  # 100 Hz update rate
gyro_bias = 0.5  # degrees/second bias (causes drift!)
gyro_noise = 0.1  # degrees/second
accel_noise = 2.0  # degrees (noisy but no drift)
alpha = 0.98  # Trust gyro 98%, accel 2%

# Initialize
gyro_angle = 0  # Gyro integration starts at 0
fused_angle = 0
gyro_only_angle = 0

print("=== Complementary Filter Demo ===")
print(f"True angle: {true_angle}°")
print(f"Gyro bias: {gyro_bias}°/s (causes {gyro_bias*10:.1f}° drift in 10s)")
print()
print(f"{'Time':>6} | {'Gyro Only':>10} | {'Accel':>10} | {'Fused':>10} | {'Error':>10}")
print("-" * 60)

for step in range(100):  # 1 second of data
    time = step * dt

    # Get sensor readings
    gyro_rate, accel_angle = simulate_imu(true_angle, gyro_bias, gyro_noise, accel_noise)

    # Integrate gyroscope (accumulates drift!)
    gyro_only_angle += gyro_rate * dt
    gyro_angle += gyro_rate * dt

    # Apply complementary filter
    fused_angle = complementary_filter(gyro_angle, accel_angle, alpha)

    # Correct gyro_angle for next iteration (key insight!)
    gyro_angle = fused_angle

    # Print every 10 steps
    if step % 10 == 0:
        error = fused_angle - true_angle
        print(f"{time:6.2f} | {gyro_only_angle:10.2f} | {accel_angle:10.2f} | {fused_angle:10.2f} | {error:+10.2f}")

print("-" * 60)
print(f"After 1s: Gyro-only drifted to {gyro_only_angle:.1f}° (error: {gyro_only_angle-true_angle:+.1f}°)")
print(f"          Fused estimate: {fused_angle:.1f}° (error: {fused_angle-true_angle:+.1f}°)")
print()
print("Key insight: Complementary filter corrects gyro drift using accelerometer!")
```

---

## 11. Real-World Robotics Applications

### 11.1 Autonomous Vehicle Sensor Suite

Self-driving cars use extensive sensor suites for robust perception:

```
         AUTONOMOUS VEHICLE SENSOR CONFIGURATION

                    3D LiDAR (roof)
                         │
                    ┌────┴────┐
                    │  360°   │
                    │ mapping │
                    └────┬────┘
                         │
    ┌────────────────────┼────────────────────┐
    │                    │                    │
Camera (front)     Camera (front)      Camera (front)
  (left)              (center)             (right)
    │                    │                    │
    └────────────────────┼────────────────────┘
                         │
              ┌──────────┼──────────┐
              │          │          │
           Radar     Radar       Radar
           (left)   (front)     (right)
              │          │          │
              └──────────┼──────────┘
                         │
            ┌────────────┼────────────┐
            │            │            │
         GPS/IMU    Wheel Encoders   Ultrasonic
         (localization)  (odometry)   (parking)
```

**Sensor Roles**:

| Sensor | Primary Function | Strengths | Weaknesses |
|--------|------------------|-----------|------------|
| LiDAR | 3D mapping, obstacle detection | Precise 3D, works in dark | Expensive, fails in fog |
| Cameras | Object recognition, lane detection | Rich information, cheap | Affected by lighting |
| Radar | Object detection, velocity | Works in all weather | Low resolution |
| GPS/IMU | Global localization | Absolute position | GPS unreliable in cities |
| Ultrasonics | Close-range obstacles | Very cheap | Short range only |

### 11.2 Humanoid Robot Sensors

Humanoid robots require comprehensive sensing for balance, manipulation, and navigation:

**Boston Dynamics Atlas**:
- Stereo cameras (head) - perception, SLAM
- LiDAR (spinning) - environment mapping
- IMU (body) - balance, orientation
- Force/torque sensors (wrists, ankles) - contact detection
- Joint encoders (all joints) - position control
- Pressure sensors (feet) - ground contact

**Key Challenge**: Fast, high-rate sensing for dynamic balance (1000+ Hz for IMU).

### 11.3 Drone Sensors

Quadcopter drones require sensors for stabilization, navigation, and obstacle avoidance:

```
              DRONE SENSOR LAYOUT

              ┌─────────────┐
              │   GPS/Mag   │ (global position, heading)
              └──────┬──────┘
                     │
    ┌────────────────┼────────────────┐
    │                │                │
Camera          IMU (center)      Barometer
(front)         (orientation)     (altitude)
    │                │                │
    └────────────────┼────────────────┘
                     │
              ┌──────┴──────┐
              │  Optical    │ (ground velocity)
              │   Flow      │
              └──────┬──────┘
                     │
    ┌────────────────┼────────────────┐
    │                │                │
Ultrasonic       LiDAR          Ultrasonic
(down)          (360°/front)    (down)
```

**Critical**: IMU runs at 400-1000 Hz for stable flight. Lower rates cause instability.

### 11.4 Robotic Manipulation

Manipulation tasks require precise sensing of object position and contact forces:

**Typical Sensor Suite**:
- RGB-D camera (workspace overview)
- Wrist-mounted camera (close-up manipulation)
- 6-axis force/torque sensor (wrist)
- Tactile sensors (fingertips)
- Joint encoders (arm)
- Joint torque sensors (collision detection)

**Application: Pick and Place**:
1. RGB-D camera detects object location
2. Wrist camera provides precise alignment
3. Force sensor detects contact
4. Tactile sensors confirm grasp
5. Force control prevents crushing

### 11.5 Mobile Robot Navigation

Indoor mobile robots use sensors for SLAM and obstacle avoidance:

```
            MOBILE ROBOT (TOP VIEW)

                  LiDAR (360°)
                     │
              ┌──────┴──────┐
              │             │
    Camera ───┤   Robot     ├─── Camera
    (left)    │    Body     │    (right)
              │             │
              │     IMU     │
              │             │
              └──────┬──────┘
                     │
              Wheel Encoders
              (left & right)
                     │
              Ultrasonic (front, rear)
```

**SLAM** (Simultaneous Localization and Mapping):
- LiDAR: Primary sensor for mapping
- Wheel encoders: Odometry (dead reckoning)
- IMU: Motion estimation between scans
- Sensor fusion: Combine all for robust localization

---

## 12. Chapter Summary

### Key Takeaways

1. **Sensors Are Essential**: Physical AI systems cannot perceive or act without sensors. They are the interface between the physical world and computation.

2. **Two Categories**:
   - **Proprioceptive**: Internal state (IMU, encoders, force sensors)
   - **Exteroceptive**: Environment (LiDAR, cameras, GPS)

3. **Key Sensor Types**:
   - **LiDAR**: Distance via time-of-flight; excellent for mapping
   - **Cameras**: Rich visual information; affected by lighting
   - **Depth Cameras**: Per-pixel distance; stereo, structured light, or ToF
   - **IMU**: Orientation and motion; prone to drift
   - **Force/Torque**: Contact sensing; essential for manipulation

4. **All Sensors Are Imperfect**:
   - Noise: Random measurement variations
   - Bias: Systematic errors (correctable via calibration)
   - Drift: Accumulating error over time (esp. gyroscopes)

5. **Signal Processing Basics**:
   - Nyquist: Sample at 2× max frequency
   - Filtering: Low-pass removes noise, high-pass removes drift
   - Tradeoffs: Smoothing vs. response time

6. **Sensor Fusion**:
   - Combine sensors for complementary strengths
   - Complementary filter: Simple, effective for IMU
   - Kalman filter: Optimal for linear Gaussian systems

7. **Calibration Is Critical**:
   - Camera intrinsics/extrinsics
   - IMU bias correction
   - Multi-sensor alignment

8. **Application-Specific Selection**:
   - Match sensors to task requirements
   - Consider range, rate, accuracy, cost, environment

### What's Next: Chapter 3 - Actuators

In Chapter 3, we complete the perception-action loop by examining **actuators**—the devices that enable Physical AI systems to act on the world:

- Types of actuators (motors, hydraulics, pneumatics, soft actuators)
- Working principles and characteristics
- Motor control basics
- Force, torque, speed, and efficiency tradeoffs
- ROS 2 actuator interfaces
- Selecting actuators for robotic applications

With sensing (Chapter 2) and actuation (Chapter 3), you'll understand both sides of the perception-action loop—the foundation for control systems (Chapter 4).

---

## 13. Glossary

| Term | Definition |
|------|------------|
| **Sensor** | Device that measures physical quantities and converts them to electrical signals |
| **Transducer** | Device that converts one form of energy to another |
| **Proprioceptive Sensor** | Sensor that measures the robot's internal state (IMU, encoders) |
| **Exteroceptive Sensor** | Sensor that measures the external environment (LiDAR, cameras) |
| **LiDAR** | Light Detection and Ranging; uses laser pulses to measure distance |
| **IMU** | Inertial Measurement Unit; combines accelerometer, gyroscope, (and magnetometer) |
| **Accelerometer** | Sensor measuring linear acceleration |
| **Gyroscope** | Sensor measuring angular velocity |
| **Magnetometer** | Sensor measuring magnetic field (used for heading) |
| **Depth Camera** | Camera that provides per-pixel distance measurements |
| **Stereo Vision** | Depth estimation using two cameras and disparity |
| **Time-of-Flight (ToF)** | Depth measurement by timing light travel |
| **Structured Light** | Depth measurement using projected patterns |
| **Force/Torque Sensor** | Measures contact forces and moments |
| **Encoder** | Measures rotation angle or position |
| **GPS** | Global Positioning System; satellite-based localization |
| **Noise** | Random errors in sensor measurements |
| **Bias** | Systematic (constant) error in sensor measurements |
| **Drift** | Accumulating error over time (especially in integrated signals) |
| **Calibration** | Process of measuring and correcting systematic sensor errors |
| **Intrinsic Parameters** | Camera internal parameters (focal length, distortion) |
| **Extrinsic Parameters** | Sensor position and orientation in world frame |
| **Sampling Rate** | Number of sensor measurements per second (Hz) |
| **Nyquist Theorem** | Sample at ≥2× max signal frequency to avoid aliasing |
| **Aliasing** | False low-frequency signal caused by undersampling |
| **Latency** | Time delay between physical event and data availability |
| **SNR** | Signal-to-Noise Ratio; measure of signal quality |
| **Low-Pass Filter** | Removes high-frequency noise, keeps slow changes |
| **High-Pass Filter** | Removes slow changes (DC, drift), keeps fast changes |
| **Sensor Fusion** | Combining multiple sensors for improved estimation |
| **Complementary Filter** | Simple fusion using frequency-domain weighting |
| **Kalman Filter** | Optimal linear estimator for Gaussian noise |
| **SLAM** | Simultaneous Localization and Mapping |
| **Point Cloud** | Set of 3D points from LiDAR or depth camera |
| **Field of View (FOV)** | Angular extent of sensor coverage |
| **Resolution** | Smallest detectable change in measured quantity |
| **Accuracy** | How close measurements are to true value |
| **Precision** | Repeatability of measurements (inverse of noise) |

---

## 14. Exercises / Review Questions

### Conceptual Questions

**Q1.** Define "sensor" in the context of Physical AI. What is the role of sensors in the perception-action loop?

**Q2.** Explain the difference between proprioceptive and exteroceptive sensors. Give two examples of each.

**Q3.** Why is sensor fusion important? Describe a scenario where using a single sensor would fail but fusion succeeds.

**Q4.** Explain why gyroscopes drift over time while accelerometers don't drift. What is the mathematical cause of gyroscope drift?

**Q5.** What is the Nyquist theorem? If a robot arm oscillates at up to 50 Hz, what is the minimum sampling rate needed?

### Technical Questions

**Q6.** A LiDAR measures a round-trip time of 66.7 nanoseconds. Calculate the distance to the object. (Speed of light: 3×10⁸ m/s)

**Q7.** A camera has the following intrinsic parameters: fx=500, fy=500, cx=320, cy=240. A 3D point at (1, 2, 5) meters in camera coordinates projects to what pixel location?

**Q8.** An IMU accelerometer has a bias of 0.1 m/s² and noise standard deviation of 0.05 m/s². After 100 measurements, what is the expected average reading if the true acceleration is 0? What is the standard error of the mean?

**Q9.** Compare stereo vision, structured light, and time-of-flight depth sensing. Create a table with: working principle, range, outdoor use, and computational requirements.

**Q10.** A complementary filter uses α=0.98 for gyroscope and (1-α)=0.02 for accelerometer. If the gyroscope reads 10°/s for 0.01 seconds (integrating to 0.1°) and the accelerometer reads 5°, what is the fused angle if the previous estimate was 4.5°?

### Practical Questions

**Q11.** You are designing a mobile robot for indoor warehouse navigation. List the sensors you would include, and justify each choice.

**Q12.** A drone's IMU runs at 400 Hz while its camera runs at 30 Hz. Explain why these different rates are appropriate for their respective uses.

**Q13.** Your robot's LiDAR occasionally returns distances of 0.0m or infinity. What physical phenomena could cause these readings? How would you handle them in software?

**Q14.** Explain the steps needed to calibrate a camera. What physical setup is required, and what parameters are you solving for?

**Q15.** Design a sensor configuration for a robotic arm that must pick up fragile eggs without breaking them. What sensors are essential, and what information does each provide?

### Coding Exercises

**Q16.** Write a Python function that simulates an encoder with quantization noise. The encoder has 1000 counts per revolution. Given a true angle in degrees, return the encoder count (with quantization).

**Q17.** Implement a moving average filter in Python that takes the last N samples. Test it on noisy data and compare N=3, N=10, and N=50.

**Q18.** Write a simple complementary filter implementation that fuses simulated gyroscope and accelerometer data. Demonstrate that it corrects drift.

### Answers (Selected)

**A5.** Nyquist theorem requires sampling at ≥2× max frequency. For 50 Hz oscillation: minimum 100 Hz. Practical systems use 200-500 Hz for safety margin.

**A6.** Distance = (c × t) / 2 = (3×10⁸ m/s × 66.7×10⁻⁹ s) / 2 = 10.0 m

**A7.** u = (fx × X / Z) + cx = (500 × 1 / 5) + 320 = 420
       v = (fy × Y / Z) + cy = (500 × 2 / 5) + 240 = 440
       Pixel location: (420, 440)

**A10.** gyro_angle = 4.5 + 0.1 = 4.6° (previous + integrated rate)
        fused = 0.98 × 4.6 + 0.02 × 5.0 = 4.508 + 0.1 = 4.608°

---

## Additional Resources

### Documentation
- ROS 2 sensor_msgs: [docs.ros.org/en/humble/p/sensor_msgs](https://docs.ros.org/en/humble/p/sensor_msgs/)
- Intel RealSense: [dev.intelrealsense.com](https://dev.intelrealsense.com/)
- Velodyne LiDAR: [velodynelidar.com](https://velodynelidar.com/)

### Books
- *Probabilistic Robotics* by Thrun, Burgard, Fox (sensor models, fusion)
- *Multiple View Geometry* by Hartley & Zisserman (camera geometry)
- *Introduction to Autonomous Mobile Robots* by Siegwart & Nourbakhsh

### Tutorials
- OpenCV Camera Calibration: [docs.opencv.org](https://docs.opencv.org/)
- ROS Navigation Stack: [navigation.ros.org](https://navigation.ros.org/)
- Kalman Filter Tutorial: [www.kalmanfilter.net](https://www.kalmanfilter.net/)

### Hardware
- Budget LiDAR: RPLidar A1 (~$100)
- Budget Depth Camera: Intel RealSense D435 (~$350)
- Budget IMU: MPU-6050 (~$5), BNO055 (~$30)

---

**STATUS: Chapter 2 Completed 100% — Ready for Publishing**

---

*Previous: [Chapter 1: Introduction to Physical AI](./01-introduction.md)*

*Next: [Chapter 3: Actuators in Physical AI](./03-actuators.md)*
